# Feuille de Route : Credit Risk Scoring Pipeline

---

## OBJECTIF DE CE DOCUMENT

Ce document est le **plan d'ex√©cution pas √† pas** du projet. Chaque √©tape doit √™tre valid√©e avant de passer √† la suivante. Cocher les cases au fur et √† mesure de l'avancement.

---

## VUE D'ENSEMBLE

```
PHASE 1          PHASE 2          PHASE 3          PHASE 4          PHASE 5          PHASE 6
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ SETUP  ‚îÇ ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ  DATA  ‚îÇ ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇFEATURE ‚îÇ ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ MODEL  ‚îÇ ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ  API   ‚îÇ ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇDEPLOI- ‚îÇ
‚îÇ        ‚îÇ      ‚îÇ  EDA   ‚îÇ      ‚îÇ  ENG.  ‚îÇ      ‚îÇ   ML   ‚îÇ      ‚îÇ  & UI  ‚îÇ      ‚îÇ EMENT  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   2 jours        3 jours        4 jours         4 jours         3 jours         2 jours
```

**Dur√©e totale estim√©e : 3 semaines**

---

## PHASE 1 : SETUP & ENVIRONNEMENT

**Objectif :** Pr√©parer l'environnement de d√©veloppement

### √âtape 1.1 : Structure du projet
- [x] Cr√©er l'arborescence compl√®te des dossiers
- [ ] Initialiser le d√©p√¥t Git
- [x] Cr√©er le fichier `.gitignore`
- [x] Cr√©er le `README.md` initial

**Structure cible :**
```
Credit_Risk_Scoring_Project/
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ Makefile
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/              # Donn√©es Kaggle brutes
‚îÇ   ‚îú‚îÄ‚îÄ processed/        # Donn√©es transform√©es
‚îÇ   ‚îî‚îÄ‚îÄ features/         # Features engineered
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_EDA.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 02_feature_engineering.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 03_modeling.ipynb
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ingestion.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocessing.py
‚îÇ   ‚îú‚îÄ‚îÄ features/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ build_features.py
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predict.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ evaluate.py
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ helpers.py
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py
‚îÇ   ‚îî‚îÄ‚îÄ endpoints/
‚îú‚îÄ‚îÄ streamlit/
‚îÇ   ‚îî‚îÄ‚îÄ app.py
‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îî‚îÄ‚îÄ dags/
‚îÇ       ‚îî‚îÄ‚îÄ credit_risk_pipeline.py
‚îú‚îÄ‚îÄ monitoring/
‚îÇ   ‚îú‚îÄ‚îÄ prometheus.yml
‚îÇ   ‚îî‚îÄ‚îÄ grafana/
‚îÇ       ‚îî‚îÄ‚îÄ dashboards/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep          # Mod√®les entra√Æn√©s (non versionn√©s)
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ test_data.py
‚îÇ   ‚îú‚îÄ‚îÄ test_features.py
‚îÇ   ‚îú‚îÄ‚îÄ test_models.py
‚îÇ   ‚îî‚îÄ‚îÄ test_api.py
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ 01_ETUDE_PROJET.md
‚îÇ   ‚îî‚îÄ‚îÄ 02_FEUILLE_DE_ROUTE.md
‚îî‚îÄ‚îÄ configs/
    ‚îî‚îÄ‚îÄ config.yaml
```

### √âtape 1.2 : Environnement Python
- [x] Cr√©er l'environnement virtuel (`python -m venv venv`)
- [x] Cr√©er `requirements.txt` avec les d√©pendances
- [x] Installer les d√©pendances (partiellement - packages EDA install√©s)

**requirements.txt initial :**
```
# Core
python>=3.10
pandas>=2.0.0
numpy>=1.24.0
scikit-learn>=1.3.0

# ML
xgboost>=2.0.0
shap>=0.43.0
optuna>=3.4.0
imbalanced-learn>=0.11.0

# Data
sqlalchemy>=2.0.0
psycopg2-binary>=2.9.0
duckdb>=0.9.0

# API
fastapi>=0.104.0
uvicorn>=0.24.0
pydantic>=2.5.0

# UI
streamlit>=1.28.0

# Orchestration
apache-airflow>=2.7.0

# Monitoring
prometheus-client>=0.18.0

# Visualization
matplotlib>=3.8.0
seaborn>=0.13.0
plotly>=5.18.0

# Utils
python-dotenv>=1.0.0
pyyaml>=6.0.0
loguru>=0.7.0

# Testing
pytest>=7.4.0
pytest-cov>=4.1.0

# Notebooks
jupyter>=1.0.0
ipykernel>=6.26.0
```

### √âtape 1.3 : Docker
- [x] Cr√©er le `Dockerfile` pour l'API
- [x] Cr√©er le `docker-compose.yml` de base
- [ ] Tester le build Docker

### √âtape 1.4 : Base de donn√©es
- [x] Configurer PostgreSQL (via Docker - fichiers pr√™ts)
- [ ] Cr√©er la base de donn√©es `credit_risk`
- [ ] Tester la connexion

**Validation Phase 1 :**
```bash
# Checklist de validation
[ ] git status fonctionne
[ ] python -c "import pandas; print(pandas.__version__)" fonctionne
[ ] docker-compose config est valide
[ ] psql -h localhost -U user -d credit_risk connecte
```

---

## PHASE 2 : DATA & EDA

**Objectif :** T√©l√©charger, explorer et comprendre les donn√©es

### √âtape 2.1 : T√©l√©chargement des donn√©es
- [x] Configurer l'API Kaggle (token KAGGLE_API_TOKEN)
- [x] T√©l√©charger le dataset Home Credit
- [x] D√©zipper dans `data/raw/`
- [x] V√©rifier l'int√©grit√© des fichiers (10 fichiers, 2.5 GB)

**Commandes :**
```bash
kaggle competitions download -c home-credit-default-risk
unzip home-credit-default-risk.zip -d data/raw/
```

### √âtape 2.2 : Exploration initiale
- [x] Cr√©er le notebook `01_EDA.ipynb`
- [x] Charger `application_train.csv` (307,511 lignes, 122 colonnes)
- [x] Analyser la distribution de TARGET (8.07% d√©fauts, ratio 1:11)
- [x] Statistiques descriptives de base
- [x] Types de donn√©es et valeurs manquantes (41 colonnes >50% manquantes)

**Points √† documenter :**
- Nombre de lignes/colonnes par table
- % de valeurs manquantes par colonne
- Distribution de la variable cible
- Corr√©lations principales

### √âtape 2.3 : EDA approfondie
- [x] Visualisation de la distribution des variables num√©riques
- [x] Analyse des variables cat√©gorielles (16 variables, cardinalit√© analys√©e)
- [x] Identification des outliers (DAYS_EMPLOYED anomalie 365243)
- [x] Analyse des corr√©lations avec TARGET (EXT_SOURCE_3: -0.179)
- [x] Exploration des tables secondaires (bureau, previous_application, etc.)

**Livrables :**
- [x] Notebook EDA complet avec visualisations
- [x] Liste des variables potentiellement importantes (EXT_SOURCE, DAYS_BIRTH, etc.)
- [x] Liste des probl√®mes identifi√©s (missing values, outliers, d√©s√©quilibre)

### √âtape 2.4 : Chargement en base de donn√©es
- [x] Cr√©er le script `src/data/ingestion.py`
- [x] D√©finir le sch√©ma des tables SQL (via init_db.sql)
- [x] Charger les donn√©es dans PostgreSQL ‚úÖ (307k + 1.7M + 1.7M = 3.7M lignes)
- [ ] Cr√©er des index pour optimiser les requ√™tes

**Approche hybride adopt√©e :**
- PostgreSQL : application_train, bureau, previous_application
- Python/CSV : installments_payments, POS_CASH_balance, credit_card_balance (agr√©gation directe)

**Validation Phase 2 :**
```bash
# Checklist de validation
[ ] Notebook EDA ex√©cutable de bout en bout
[ ] SELECT COUNT(*) FROM application_train retourne 307511
[ ] Toutes les tables sont charg√©es dans PostgreSQL
[ ] Document avec les insights principaux de l'EDA
```

---

## PHASE 3 : FEATURE ENGINEERING

**Objectif :** Cr√©er des variables pertinentes pour le mod√®le

### √âtape 3.1 : Nettoyage des donn√©es ‚úÖ
- [x] Cr√©er `src/data/preprocessing.py`
- [x] G√©rer les valeurs manquantes (imputation m√©diane/mode)
- [x] G√©rer les outliers (capping par percentiles)
- [x] Corriger les anomalies identifi√©es dans l'EDA (DAYS_EMPLOYED = 365243)

**Classe DataPreprocessor cr√©√©e avec :**
- `fix_anomalies()` : Correction DAYS_EMPLOYED + indicateur binaire
- `impute_missing_values()` : M√©diane (num) / Mode (cat)
- `cap_outliers()` : Percentiles 1%-99%
- `optimize_dtypes()` : R√©duction m√©moire automatique

**Strat√©gies de gestion des valeurs manquantes :**
| Type de variable | Strat√©gie |
|-----------------|-----------|
| Num√©rique | M√©diane ou -999 (indicateur explicite) |
| Cat√©gorielle | Mode ou cat√©gorie "Unknown" |
| Binaire | Mode |

### √âtape 3.2 : Agr√©gation des tables secondaires ‚úÖ
- [x] Cr√©er `src/features/build_features.py`
- [x] Agr√©ger `bureau` ‚Üí m√©triques par client (SQL PostgreSQL)
- [x] Agr√©ger `previous_application` ‚Üí m√©triques par client (SQL PostgreSQL)
- [x] Agr√©ger `installments_payments.csv` ‚Üí comportement de paiement (Python chunked)
- [x] Agr√©ger `credit_card_balance.csv` ‚Üí utilisation carte (Python chunked)
- [x] Agr√©ger `POS_CASH_balance.csv` ‚Üí comportement POS (Python chunked)

**Classe FeatureEngineer cr√©√©e avec :**
- `create_application_features()` : 16 features (ratios, √¢ge, documents)
- `create_bureau_features()` : ~18 features depuis PostgreSQL
- `create_previous_application_features()` : ~18 features depuis PostgreSQL
- `create_installments_features()` : ~14 features (chunked CSV)
- `create_pos_cash_features()` : ~17 features (chunked CSV)
- `create_credit_card_features()` : ~21 features (chunked CSV)

**Exemples d'agr√©gations :**
```python
# Bureau
- Nombre de cr√©dits actifs
- Montant total des cr√©dits
- Nombre de retards pass√©s
- Dur√©e moyenne des cr√©dits

# Previous Application
- Nombre de demandes pr√©c√©dentes
- Taux d'acceptation
- Montant moyen demand√© vs accord√©
- Type de cr√©dit le plus fr√©quent

# Installments
- Nombre de paiements en retard
- Montant moyen des retards
- Ratio paiements √† temps
```

### √âtape 3.3 : Cr√©ation de nouvelles features ‚úÖ
- [x] Ratios financiers (credit_income_ratio, annuity_income_ratio, etc.)
- [x] Indicateurs d√©riv√©s (bureau_active_ratio, prev_approval_rate, etc.)
- [x] Features comportementales (instal_late_ratio, cc_utilization_mean, etc.)
- [x] Agr√©gats statistiques (mean, max, sum, count par client)

**Features prioritaires √† cr√©er :**
```python
# Ratios
ANNUITY_INCOME_RATIO = AMT_ANNUITY / AMT_INCOME_TOTAL
CREDIT_INCOME_RATIO = AMT_CREDIT / AMT_INCOME_TOTAL
CREDIT_ANNUITY_RATIO = AMT_CREDIT / AMT_ANNUITY
GOODS_CREDIT_RATIO = AMT_GOODS_PRICE / AMT_CREDIT

# √Çge
DAYS_BIRTH_YEARS = -DAYS_BIRTH / 365
DAYS_EMPLOYED_YEARS = -DAYS_EMPLOYED / 365
EMPLOYED_TO_BIRTH_RATIO = DAYS_EMPLOYED / DAYS_BIRTH

# Indicateurs
HAS_BUREAU = 1 if client in bureau else 0
HAS_PREVIOUS = 1 if client in previous_application else 0
```

### √âtape 3.4 : Encodage des variables cat√©gorielles
- [x] Variables cat√©gorielles conserv√©es ‚Üí **Report√© volontairement √† Phase 4**

**Raison du report :**
- XGBoost supporte les cat√©gorielles nativement (`enable_categorical=True`)
- Certaines variables ont haute cardinalit√© (ORGANIZATION_TYPE: 58 valeurs) ‚Üí one-hot non viable
- Approche moderne : encodage dans le pipeline de mod√©lisation
- C'est l'approche des top Kagglers sur Home Credit

### √âtape 3.5 : S√©lection de features
- [x] NaN remplis (0 pour count/sum, m√©diane pour autres)
- [x] S√©lection finale ‚Üí **Report√©e √† Phase 4** (apr√®s analyse importance features)

### √âtape 3.6 : Dataset final ‚úÖ
- [x] Merger toutes les features dans un dataset unique
- [x] Sauvegarder dans `data/features/features_v1.csv`
- [x] 307,511 lignes √ó 225 colonnes (452.3 MB)

**R√©sum√© features cr√©√©es :**
| Source | Nb features |
|--------|-------------|
| application | 16 |
| bureau | 18 |
| previous_application | 18 |
| installments | 13 |
| pos_cash | 17 |
| credit_card | 21 |
| **Total nouvelles** | **103** |

**Validation Phase 3 :**
```bash
# Checklist de validation
[x] Script de feature engineering reproductible (build_features.py)
[x] Dataset final avec 225 colonnes document√©es
[ ] Notebook 02_feature_engineering.ipynb (optionnel - code dans src/)
[x] Aucune fuite de donn√©es (target leakage) - features bas√©es sur historique
```

---

## PHASE 4 : MOD√âLISATION ML

**Objectif :** Entra√Æner et √©valuer le mod√®le de scoring

### √âtape 4.1 : Pr√©paration des donn√©es
- [ ] Cr√©er `src/models/train.py`
- [ ] Charger `data/features/features_v1.csv`
- [ ] **Encodage des variables cat√©gorielles** (report√© de Phase 3) :
  - [ ] Identifier les colonnes cat√©gorielles (type object)
  - [ ] Convertir en type `category` pour XGBoost (`enable_categorical=True`)
  - [ ] Ou Label Encoding si n√©cessaire
- [ ] Split train/validation/test (70/15/15)
- [ ] G√©rer le d√©s√©quilibre de classes (`scale_pos_weight` ~11)
- [ ] **S√©lection de features** (report√©e de Phase 3) :
  - [ ] Analyser importance apr√®s baseline
  - [ ] Supprimer features √† faible importance si n√©cessaire

**Strat√©gie pour le d√©s√©quilibre :**
```python
# Option 1: XGBoost scale_pos_weight
scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()

# Option 2: SMOTE (√† utiliser avec pr√©caution)
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)
```

### √âtape 4.2 : Baseline model
- [ ] Entra√Æner un mod√®le XGBoost avec param√®tres par d√©faut
- [ ] √âvaluer sur validation set
- [ ] Documenter les m√©triques de base

**M√©triques √† calculer :**
- AUC-ROC
- Gini (2*AUC - 1)
- Precision, Recall, F1-score
- Matrice de confusion
- Courbe ROC et Precision-Recall

### √âtape 4.3 : Hyperparameter tuning
- [ ] D√©finir l'espace de recherche des hyperparam√®tres
- [ ] Utiliser Optuna pour l'optimisation
- [ ] Validation crois√©e 5-fold
- [ ] S√©lectionner le meilleur mod√®le

**Param√®tres √† tuner :**
```python
param_space = {
    'max_depth': [3, 5, 7, 9],
    'learning_rate': [0.01, 0.05, 0.1],
    'n_estimators': [100, 200, 500],
    'min_child_weight': [1, 3, 5],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9],
    'gamma': [0, 0.1, 0.2],
    'reg_alpha': [0, 0.1, 1],
    'reg_lambda': [1, 2, 5]
}
```

### √âtape 4.4 : √âvaluation finale
- [ ] Cr√©er `src/models/evaluate.py`
- [ ] √âvaluer sur le test set (jamais utilis√© avant)
- [ ] G√©n√©rer toutes les m√©triques et visualisations
- [ ] Analyser les erreurs (faux positifs, faux n√©gatifs)

### √âtape 4.5 : Explicabilit√© SHAP
- [ ] Calculer les SHAP values
- [ ] Feature importance globale
- [ ] Graphiques : summary plot, dependence plots
- [ ] Explications individuelles pour quelques exemples

**Visualisations SHAP √† cr√©er :**
```python
# Global
shap.summary_plot(shap_values, X_test)
shap.summary_plot(shap_values, X_test, plot_type="bar")

# Individuel
shap.force_plot(explainer.expected_value, shap_values[0], X_test.iloc[0])
shap.waterfall_plot(shap_values[0])
```

### √âtape 4.6 : Sauvegarde du mod√®le
- [ ] Cr√©er `src/models/predict.py`
- [ ] Sauvegarder le mod√®le (joblib ou pickle)
- [ ] Sauvegarder les objets de preprocessing (encoders, scalers)
- [ ] Documenter la version et les performances

**Validation Phase 4 :**
```bash
# Checklist de validation
[ ] AUC-ROC > 0.75 sur le test set
[ ] Notebook 03_modeling.ipynb complet
[ ] Mod√®le sauvegard√© dans models/
[ ] Visualisations SHAP g√©n√©r√©es
[ ] M√©triques document√©es
```

---

## PHASE 5 : API & INTERFACE

**Objectif :** Exposer le mod√®le via une API et cr√©er une interface d√©mo

### √âtape 5.1 : API FastAPI
- [ ] Cr√©er `api/main.py`
- [ ] Cr√©er `api/schemas.py` (Pydantic models)
- [ ] Endpoint `/predict` pour le scoring
- [ ] Endpoint `/explain` pour les SHAP values
- [ ] Endpoint `/health` pour le monitoring

**Endpoints √† impl√©menter :**
```
POST /predict
  Input: donn√©es client (JSON)
  Output: {score: float, probability: float, decision: str}

POST /explain
  Input: donn√©es client (JSON)
  Output: {shap_values: dict, top_features: list}

GET /health
  Output: {status: "healthy", model_version: str}

GET /metrics
  Output: m√©triques Prometheus
```

### √âtape 5.2 : Tests API
- [ ] Cr√©er `tests/test_api.py`
- [ ] Tests unitaires des endpoints
- [ ] Tests de validation des inputs
- [ ] Tests de performance (latence)

### √âtape 5.3 : Interface Streamlit
- [ ] Cr√©er `streamlit/app.py`
- [ ] Formulaire de saisie des donn√©es client
- [ ] Affichage du score et de la d√©cision
- [ ] Visualisation SHAP interactive
- [ ] Exemples pr√©-remplis pour la d√©mo

**Sections de l'interface :**
```
1. Header avec titre et description
2. Sidebar avec exemples de clients
3. Formulaire de saisie (ou upload CSV)
4. R√©sultat : Score, Probabilit√©, D√©cision
5. Graphique SHAP (explication)
6. Top 10 facteurs influents
```

### √âtape 5.4 : Int√©gration API ‚Üî Streamlit
- [ ] Streamlit appelle l'API FastAPI
- [ ] Gestion des erreurs
- [ ] Loading states

**Validation Phase 5 :**
```bash
# Checklist de validation
[ ] curl localhost:8000/predict fonctionne
[ ] Tests API passent (pytest tests/test_api.py)
[ ] Streamlit s'affiche correctement
[ ] Latence < 500ms
```

---

## PHASE 6 : ORCHESTRATION & MONITORING

**Objectif :** Automatiser le pipeline et monitorer l'application

### √âtape 6.1 : DAG Airflow
- [ ] Cr√©er `airflow/dags/credit_risk_pipeline.py`
- [ ] Task 1 : Ingestion des donn√©es
- [ ] Task 2 : Feature engineering
- [ ] Task 3 : Entra√Ænement du mod√®le (si schedule)
- [ ] Task 4 : Validation des m√©triques

**Structure du DAG :**
```python
# credit_risk_pipeline.py
ingest >> preprocess >> feature_eng >> train >> evaluate >> deploy
```

### √âtape 6.2 : Monitoring Prometheus
- [ ] Configurer `monitoring/prometheus.yml`
- [ ] Ajouter les m√©triques dans FastAPI
- [ ] M√©triques : requ√™tes/sec, latence, erreurs

**M√©triques √† exposer :**
```python
# Compteurs
requests_total
predictions_total
errors_total

# Histogrammes
prediction_latency_seconds

# Gauges
model_version
last_prediction_timestamp
```

### √âtape 6.3 : Dashboard Grafana
- [ ] Configurer Grafana (via Docker)
- [ ] Cr√©er le dashboard de monitoring
- [ ] Panels : latence, throughput, erreurs

### √âtape 6.4 : Docker Compose complet
- [ ] Int√©grer tous les services dans docker-compose.yml
- [ ] PostgreSQL
- [ ] API FastAPI
- [ ] Streamlit
- [ ] Airflow
- [ ] Prometheus
- [ ] Grafana

**docker-compose.yml final :**
```yaml
services:
  postgres:
    image: postgres:15
  api:
    build: ./api
  streamlit:
    build: ./streamlit
  airflow:
    image: apache/airflow:2.7.0
  prometheus:
    image: prom/prometheus
  grafana:
    image: grafana/grafana
```

**Validation Phase 6 :**
```bash
# Checklist de validation
[ ] docker-compose up d√©marre tous les services
[ ] DAG Airflow visible dans l'UI
[ ] Dashboard Grafana affiche les m√©triques
[ ] Tout fonctionne ensemble
```

---

## PHASE 7 : FINALISATION & D√âPLOIEMENT

**Objectif :** D√©ployer publiquement et documenter

### √âtape 7.1 : Documentation
- [ ] README.md complet avec :
  - Description du projet
  - Architecture
  - Installation
  - Usage
  - R√©sultats
  - Screenshots
- [ ] Docstrings dans le code
- [ ] Commentaires pour les parties complexes

### √âtape 7.2 : D√©ploiement Streamlit Cloud
- [ ] Cr√©er le repo GitHub public
- [ ] Connecter √† Streamlit Cloud
- [ ] Configurer les secrets
- [ ] Tester l'app d√©ploy√©e

### √âtape 7.3 : Post LinkedIn
- [ ] R√©diger le post
- [ ] Screenshots/GIF de l'app
- [ ] Lien vers le repo et la d√©mo
- [ ] Hashtags pertinents

**Validation Finale :**
```bash
# Checklist finale
[ ] Repo GitHub public et propre
[ ] README professionnel
[ ] App Streamlit accessible en ligne
[ ] docker-compose up fonctionne en local
[ ] Post LinkedIn publi√©
```

---

## SUIVI DE PROGRESSION

| Phase | Status | Date d√©but | Date fin | Notes |
|-------|--------|------------|----------|-------|
| Phase 1 : Setup | ‚úÖ Termin√© | 25/01/2026 | 25/01/2026 | Structure, requirements, Docker configur√©s |
| Phase 2 : Data & EDA | ‚úÖ Termin√© | 25/01/2026 | 26/01/2026 | EDA ‚úÖ, PostgreSQL: 3.7M lignes charg√©es |
| Phase 3 : Feature Engineering | ‚úÖ Termin√© | 26/01/2026 | 26/01/2026 | 103 features cr√©√©es, dataset 225 colonnes |
| Phase 4 : Mod√©lisation | ‚¨ú √Ä faire | | | |
| Phase 5 : API & UI | ‚¨ú √Ä faire | | | |
| Phase 6 : Orchestration | ‚¨ú √Ä faire | | | |
| Phase 7 : D√©ploiement | ‚¨ú √Ä faire | | | |

**L√©gende :** ‚¨ú √Ä faire | üîÑ En cours | ‚úÖ Termin√© | ‚ùå Bloqu√©

---

## NOTES ET D√âCISIONS

*(Espace pour noter les d√©cisions prises et les changements de plan)*

| Date | D√©cision | Raison |
|------|----------|--------|
| 25/01/2026 | PostgreSQL comme BDD principale | Performance sur jointures, simulation production |
| 25/01/2026 | DuckDB en alternative si ressources limit√©es | Flexibilit√© |
| 25/01/2026 | scale_pos_weight pour d√©s√©quilibre | Ratio 1:11, SMOTE trop lourd sur 300k lignes |
| 25/01/2026 | EXT_SOURCE variables prioritaires | Corr√©lations les plus fortes (-0.18) |
| 25/01/2026 | Jupyter navigateur plut√¥t que VS Code | Probl√®me extension Jupyter VS Code |
| 26/01/2026 | Approche hybride pour l'ingestion | Tables principales en PostgreSQL, grosses tables agr√©g√©es en Python |
| 26/01/2026 | R√©duction chunk_size √† 10000 | √âviter crash m√©moire WSL (√©tait 50000) |
| 26/01/2026 | PostgreSQL local (pas Docker) | Port 5432 d√©j√† utilis√© par PostgreSQL syst√®me |
| 27/01/2026 | Encodage cat√©gorielles report√© √† Phase 4 | XGBoost supporte `enable_categorical=True`, plus flexible |
| 27/01/2026 | Notebook pour Phase 4 (pas scripts) | Graphiques interactifs, SHAP plots, exploration |
| 27/01/2026 | Jupyter navigateur (pas VS Code) | Probl√®me kernel VS Code, acc√®s direct sans token |

---

**Document cr√©√© le :** Janvier 2026
**Derni√®re mise √† jour :** 27 Janvier 2026
