{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Modélisation Credit Risk Scoring\n",
    " \n",
    "**Phase:** 4 - Modélisation ML\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "1. Charger le dataset de features (`features_v1.csv`)\n",
    "2. Encoder les variables catégorielles\n",
    "3. Split train/validation/test (70/15/15)\n",
    "4. Entraîner un baseline XGBoost\n",
    "5. Tuner les hyperparamètres avec Optuna\n",
    "6. Évaluer le modèle final\n",
    "7. Analyser avec SHAP\n",
    "\n",
    "**Objectif performance:** AUC-ROC > 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, \n",
    "    precision_recall_curve, average_precision_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    f1_score, precision_score, recall_score\n",
    ")\n",
    "import xgboost as xgb\n",
    "\n",
    "# Tuning\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# Explainability\n",
    "import shap\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utils\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "pd.set_option('display.max_columns', 50)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset de features\n",
    "DATA_PATH = Path('../data/features/features_v1.csv')\n",
    "\n",
    "print(f\"Chargement de {DATA_PATH}...\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Mémoire: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier la distribution de la cible\n",
    "print(\"Distribution TARGET:\")\n",
    "print(df['target'].value_counts())\n",
    "print(f\"\\nRatio défaut: {df['target'].mean()*100:.2f}%\")\n",
    "print(f\"Ratio classe (neg/pos): {(df['target']==0).sum() / (df['target']==1).sum():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Identifier les types de colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier les colonnes par type\n",
    "id_col = 'sk_id_curr'\n",
    "target_col = 'target'\n",
    "\n",
    "# Colonnes catégorielles (type object)\n",
    "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Colonnes numériques\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "num_cols = [c for c in num_cols if c not in [id_col, target_col]]\n",
    "\n",
    "print(f\"ID: {id_col}\")\n",
    "print(f\"Target: {target_col}\")\n",
    "print(f\"Colonnes catégorielles: {len(cat_cols)}\")\n",
    "print(f\"Colonnes numériques: {len(num_cols)}\")\n",
    "\n",
    "if cat_cols:\n",
    "    print(f\"\\nCatégorielles: {cat_cols[:10]}...\" if len(cat_cols) > 10 else f\"\\nCatégorielles: {cat_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Encodage des variables catégorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cardinalité des variables catégorielles\n",
    "if cat_cols:\n",
    "    cardinality = {col: df[col].nunique() for col in cat_cols}\n",
    "    cardinality_df = pd.DataFrame.from_dict(cardinality, orient='index', columns=['unique_values'])\n",
    "    cardinality_df = cardinality_df.sort_values('unique_values', ascending=False)\n",
    "    print(\"Cardinalité des variables catégorielles:\")\n",
    "    print(cardinality_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder les catégorielles avec LabelEncoder\n",
    "label_encoders = {}\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    # Gérer les NaN\n",
    "    df[col] = df[col].fillna('MISSING')\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    \n",
    "print(f\"Encodées: {len(cat_cols)} colonnes\")\n",
    "\n",
    "# Vérifier qu'il n'y a plus de colonnes object\n",
    "remaining_object = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Colonnes object restantes: {remaining_object}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Préparation X et y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer features et target\n",
    "feature_cols = [c for c in df.columns if c not in [id_col, target_col]]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[target_col]\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Split Train / Validation / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 70% train / 15% validation / 15% test\n",
    "# Stratified pour garder le ratio de classes\n",
    "\n",
    "# D'abord: 70% train, 30% temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.30, \n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Ensuite: 50% val, 50% test (du temp = 15% chacun du total)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.50,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Split effectué:\")\n",
    "print(f\"  Train: {X_train.shape[0]:,} ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Valid: {X_val.shape[0]:,} ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Test:  {X_test.shape[0]:,} ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nRatio défaut par set:\")\n",
    "print(f\"  Train: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"  Valid: {y_val.mean()*100:.2f}%\")\n",
    "print(f\"  Test:  {y_test.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer scale_pos_weight pour gérer le déséquilibre\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"scale_pos_weight: {scale_pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline XGBoost avec paramètres par défaut\n",
    "baseline_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='auc'\n",
    ")\n",
    "\n",
    "print(\"Entraînement du baseline...\")\n",
    "baseline_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Baseline entraîné!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation baseline sur validation\n",
    "y_pred_proba_baseline = baseline_model.predict_proba(X_val)[:, 1]\n",
    "auc_baseline = roc_auc_score(y_val, y_pred_proba_baseline)\n",
    "gini_baseline = 2 * auc_baseline - 1\n",
    "\n",
    "print(\"=\" * 40)\n",
    "print(\"BASELINE RESULTS (Validation Set)\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"AUC-ROC: {auc_baseline:.4f}\")\n",
    "print(f\"Gini:    {gini_baseline:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Hyperparameter Tuning avec Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Fonction objectif pour Optuna.\"\"\"\n",
    "    \n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "        'eval_metric': 'auc'\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancer l'optimisation Optuna\n",
    "# Ajuster n_trials selon le temps disponible (100 = ~30-60 min)\n",
    "\n",
    "N_TRIALS = 100  # Modifier si besoin\n",
    "\n",
    "print(f\"Démarrage Optuna avec {N_TRIALS} trials...\")\n",
    "print(\"Cela peut prendre 30-60 minutes.\\n\")\n",
    "\n",
    "sampler = TPESampler(seed=RANDOM_STATE)\n",
    "study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "\n",
    "# Callback pour afficher la progression\n",
    "def print_callback(study, trial):\n",
    "    if trial.number % 10 == 0:\n",
    "        print(f\"Trial {trial.number}: AUC = {trial.value:.4f} (Best: {study.best_value:.4f})\")\n",
    "\n",
    "study.optimize(objective, n_trials=N_TRIALS, callbacks=[print_callback], show_progress_bar=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"OPTUNA TERMINÉ\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Meilleur AUC: {study.best_value:.4f}\")\n",
    "print(f\"Amélioration vs baseline: +{(study.best_value - auc_baseline)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meilleurs paramètres\n",
    "print(\"Meilleurs paramètres:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Modèle Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner le modèle final avec les meilleurs paramètres\n",
    "best_params = study.best_params\n",
    "best_params['scale_pos_weight'] = scale_pos_weight\n",
    "best_params['random_state'] = RANDOM_STATE\n",
    "best_params['n_jobs'] = -1\n",
    "best_params['eval_metric'] = 'auc'\n",
    "\n",
    "final_model = xgb.XGBClassifier(**best_params)\n",
    "\n",
    "print(\"Entraînement du modèle final...\")\n",
    "final_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Modèle final entraîné!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Évaluation Finale (Test Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions sur le test set (jamais vu pendant l'entraînement)\n",
    "y_pred_proba_test = final_model.predict_proba(X_test)[:, 1]\n",
    "y_pred_test = (y_pred_proba_test >= 0.5).astype(int)\n",
    "\n",
    "# Métriques\n",
    "auc_test = roc_auc_score(y_test, y_pred_proba_test)\n",
    "gini_test = 2 * auc_test - 1\n",
    "precision_test = precision_score(y_test, y_pred_test)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"RÉSULTATS FINAUX (Test Set - Données jamais vues)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"AUC-ROC:   {auc_test:.4f}\")\n",
    "print(f\"Gini:      {gini_test:.4f}\")\n",
    "print(f\"Precision: {precision_test:.4f}\")\n",
    "print(f\"Recall:    {recall_test:.4f}\")\n",
    "print(f\"F1-Score:  {f1_test:.4f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if auc_test >= 0.75:\n",
    "    print(\"\\n>>> OBJECTIF ATTEINT: AUC > 0.75 <<<\")\n",
    "else:\n",
    "    print(f\"\\n>>> Objectif non atteint. Manque: {0.75 - auc_test:.4f} <<<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Pas de défaut', 'Défaut'],\n",
    "            yticklabels=['Pas de défaut', 'Défaut'])\n",
    "plt.xlabel('Prédit')\n",
    "plt.ylabel('Réel')\n",
    "plt.title('Matrice de Confusion - Test Set')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=['Pas de défaut', 'Défaut']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbe ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, 'b-', linewidth=2, label=f'Modèle (AUC = {auc_test:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'r--', linewidth=1, label='Random (AUC = 0.5)')\n",
    "plt.xlabel('Taux de Faux Positifs (FPR)')\n",
    "plt.ylabel('Taux de Vrais Positifs (TPR)')\n",
    "plt.title('Courbe ROC - Credit Risk Scoring')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbe Precision-Recall\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba_test)\n",
    "ap = average_precision_score(y_test, y_pred_proba_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall_curve, precision_curve, 'g-', linewidth=2, label=f'Modèle (AP = {ap:.4f})')\n",
    "plt.axhline(y=y_test.mean(), color='r', linestyle='--', label=f'Baseline ({y_test.mean():.4f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Courbe Precision-Recall')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Explicabilité SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les SHAP values (peut prendre quelques minutes)\n",
    "print(\"Calcul des SHAP values...\")\n",
    "\n",
    "# Utiliser un échantillon pour accélérer\n",
    "X_sample = X_test.sample(n=min(1000, len(X_test)), random_state=RANDOM_STATE)\n",
    "\n",
    "explainer = shap.TreeExplainer(final_model)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "print(\"SHAP values calculés!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance globale (SHAP)\n",
    "plt.figure(figsize=(12, 10))\n",
    "shap.summary_plot(shap_values, X_sample, plot_type='bar', show=False, max_display=20)\n",
    "plt.title('Top 20 Features - Importance SHAP')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot (impact sur la prédiction)\n",
    "plt.figure(figsize=(12, 10))\n",
    "shap.summary_plot(shap_values, X_sample, show=False, max_display=20)\n",
    "plt.title('Impact des Features sur la Prédiction')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 features les plus importantes\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_sample.columns,\n",
    "    'importance': np.abs(shap_values).mean(axis=0)\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Features:\")\n",
    "print(feature_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Sauvegarde du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le dossier models si nécessaire\n",
    "MODELS_PATH = Path('../models')\n",
    "MODELS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "model_path = MODELS_PATH / 'xgboost_credit_risk_v1.pkl'\n",
    "joblib.dump(final_model, model_path)\n",
    "print(f\"Modèle sauvegardé: {model_path}\")\n",
    "\n",
    "# Sauvegarder les noms de features\n",
    "features_path = MODELS_PATH / 'feature_names.json'\n",
    "with open(features_path, 'w') as f:\n",
    "    json.dump(feature_cols, f)\n",
    "print(f\"Features sauvegardées: {features_path}\")\n",
    "\n",
    "# Sauvegarder les label encoders\n",
    "encoders_path = MODELS_PATH / 'label_encoders.pkl'\n",
    "joblib.dump(label_encoders, encoders_path)\n",
    "print(f\"Encoders sauvegardés: {encoders_path}\")\n",
    "\n",
    "# Sauvegarder les métriques\n",
    "metrics = {\n",
    "    'auc_roc': float(auc_test),\n",
    "    'gini': float(gini_test),\n",
    "    'precision': float(precision_test),\n",
    "    'recall': float(recall_test),\n",
    "    'f1_score': float(f1_test),\n",
    "    'best_params': best_params\n",
    "}\n",
    "metrics_path = MODELS_PATH / 'metrics.json'\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics, f, indent=2, default=str)\n",
    "print(f\"Métriques sauvegardées: {metrics_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Résumé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RÉSUMÉ - PHASE 4 MODÉLISATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataset: {df.shape[0]:,} lignes × {len(feature_cols)} features\")\n",
    "print(f\"\\nSplit:\")\n",
    "print(f\"  - Train: {len(X_train):,}\")\n",
    "print(f\"  - Valid: {len(X_val):,}\")\n",
    "print(f\"  - Test:  {len(X_test):,}\")\n",
    "print(f\"\\nRésultats:\")\n",
    "print(f\"  - Baseline AUC: {auc_baseline:.4f}\")\n",
    "print(f\"  - Final AUC:    {auc_test:.4f}\")\n",
    "print(f\"  - Amélioration: +{(auc_test - auc_baseline)*100:.2f}%\")\n",
    "print(f\"\\nTop 5 Features:\")\n",
    "for i, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "print(f\"\\nFichiers sauvegardés:\")\n",
    "print(f\"  - {model_path}\")\n",
    "print(f\"  - {features_path}\")\n",
    "print(f\"  - {encoders_path}\")\n",
    "print(f\"  - {metrics_path}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 4 TERMINÉE\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
